<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Student projects | Mobile Perception Systems Lab</title><link>https://tue-mps.github.io/post/</link><atom:link href="https://tue-mps.github.io/post/index.xml" rel="self" type="application/rss+xml"/><description>Student projects</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 25 Aug 2021 00:00:00 +0000</lastBuildDate><image><url>https://tue-mps.github.io/media/icon_hu9bffa56b6a2f6e08d28b87009a0bc873_1397_512x512_fill_lanczos_center_3.png</url><title>Student projects</title><link>https://tue-mps.github.io/post/</link></image><item><title>To Style or Not to Style? A Comparison Between Data Augmentation and Style Supression for Domain Generalization</title><link>https://tue-mps.github.io/post/2021-09-internship-style-domain-generalization/</link><pubDate>Wed, 25 Aug 2021 00:00:00 +0000</pubDate><guid>https://tue-mps.github.io/post/2021-09-internship-style-domain-generalization/</guid><description>&lt;p>&lt;strong>Important note: this project is provisional, and the details may be subject to change.&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Summary:&lt;/strong> During this project, you will use an existing network for semantic segmentation and study whether its generalization capacity can be improved by performing either data augmentation or style supression.&lt;/p>
&lt;p>&lt;strong>Type:&lt;/strong> Internship&lt;/p>
&lt;p>&lt;strong>Starting date:&lt;/strong> September 2021&lt;/p>
&lt;p>&lt;strong>Supervisor:&lt;/strong> Fabrizio J. Piva&lt;/p>
&lt;p>&lt;strong>General description:&lt;/strong>&lt;/p>
&lt;p>When deep neural networks are trained on a given dataset, there is the assumption that the training and evaluation data are Independent and Identically Distributed (IID), i.e., both sets are independent and drawn from the same underlying distribution. This means that if we wanted to use a deep neural network for a real-world application, we would need to manually label lots of data from that particular scenario in order to build a proper training (and validation) set. However, in dense prediction tasks such as semantic segmentation, labeling large scale data becomes unbearable, and publicly available datasets very likely do not to comply to the IID assumption. On the other hand, ignoring this assumption would make the neural network unusable due to its low performance caused by the large difference between training and evaluation data.&lt;/p>
&lt;p>In order to solve this problem, domain generalization methods aim to build and train a neural network with the goal of increasing the performance not only in the (seen) training dataset, but also on multiple (unseen) domains. Recently, several methods for domain generalization in semantic segmentation propose either to 1) augment the training data by creating multiple alternative representations with different style [1], or 2) supress the style component by descomposing the images in style and content, training only with the content [2]. Each approach has its strengths and weaknesses, but to this date no works have made a thorough comparison between these techniques.&lt;/p>
&lt;p>Thus, the research question for this proposal is: to style or not to style? Throughout this internship you will find out the answer.&lt;/p>
&lt;p>&lt;strong>Task description:&lt;/strong>
In this project, you will use a standard semantic segmentation network (for instance, DeepLab v2 [3]) and implement style-based data augmentations as well as style supression techniques for domain generalization. This requires:&lt;/p>
&lt;ul>
&lt;li>Implementing a relatively fast style transfer method, extending it to multiple representations, ideally without reference image.&lt;/li>
&lt;li>Training the semantic segmentation network with these multiple representations.&lt;/li>
&lt;li>Adapt a style supression technique to the semantic segmentation network.&lt;/li>
&lt;li>Training the semantic segmentation network without the style component.&lt;/li>
&lt;li>Evaluating the resulting models on multiple datasets: seen as well as unseen.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Prerequisites:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Theoretical knowledge about deep neural networks for computer vision.&lt;/li>
&lt;li>Experience with implementing a deep neural network for computer vision.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Interested?&lt;/strong> Send an email to &lt;a href="mailto:f.j.piva@tue.nl">f.j.piva@tue.nl&lt;/a>, containing:&lt;/p>
&lt;ul>
&lt;li>Brief motivation letter&lt;/li>
&lt;li>List of relevant courses and grades&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>References&lt;/strong>&lt;/p>
&lt;p>[1] X. Yue, Y. Zhang, S. Zhao, A. Sangiovanni-Vincentelli, K. Keutzer and B. Gong, &amp;ldquo;Domain Randomization and Pyramid Consistency: Simulation-to-Real Generalization Without Accessing Target Domain Data&amp;rdquo;, IEEE/CVF International Conference on Computer Vision (ICCV), 2019, pp. 2100-2110.&lt;/p>
&lt;p>[2] Choi, Sungha, Sanghun Jung, Huiwon Yun, J. Kim, Seungryong Kim and J. Choo. &amp;ldquo;RobustNet: Improving Domain Generalization in Urban-Scene Segmentation via Instance Selective Whitening.&amp;rdquo; ArXiv abs/2103.15597, 2021.&lt;/p>
&lt;p>[3] L. C. Chen, G. Papandreou, I. Kokkinos, K. Murphy and A. L. Yuille, &amp;ldquo;DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs&amp;rdquo;, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 40, no. 4, pp. 834-848, 2018.&lt;/p></description></item><item><title>Teach a driving agent what it cannot see</title><link>https://tue-mps.github.io/post/2021-09-internship-risk-aware-decision-making/</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://tue-mps.github.io/post/2021-09-internship-risk-aware-decision-making/</guid><description>&lt;p>&lt;strong>Important note: this project is provisional, and the details may be subject to change.&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Summary:&lt;/strong> During this project, you will implement, train and evaluate a Reinforcement Learning method for driving in a 3D simulation enviornment&lt;/p>
&lt;p>&lt;strong>Type:&lt;/strong> Internship&lt;/p>
&lt;p>&lt;strong>Starting date:&lt;/strong> September 2021&lt;/p>
&lt;p>&lt;strong>Supervisor:&lt;/strong> Arash Roomi Zadeh&lt;/p>
&lt;p>&lt;strong>General description:&lt;/strong>&lt;/p>
&lt;p>Safe navigation in urban environments remains a challenging problem for autonomous vehicles. Reinforcement Learning (RL) algorithms have shown great success in many behavior generation applications when the agent has access to a reliable state signal such as video game playing tasks. This motivates researchers to also use RL for decision-making for automated driving. Despite these achievements, the state-of-the-art RL methods for automated driving still cannot achieve safety requirements for real-world autonomous driving. This is due to both non-practicality of training in the real world, the uncertain nature of the observations, and the stochastic nature of the driving environment.&lt;/p>
&lt;p>Traditionally, RL algorithms choose actions that maximize mean return value. This is desired in domains in which failure has a low impact. However, in real-world driving tasks, events with low probability but a high impact (e.g., rare collision types) also play a critical role in policy generation. This motivates using risk evaluation for policy generation for decision-making. Risk-sensitive optimization of distributional DQN networks has been employed in fields in which risk-avoidance is critical and has got promising results.&lt;/p>
&lt;p>The first goal of this internship is to implement a Distributional Reinforcement Learning algorithm and use it for driving policy risk evaluation. Secondly, the aim is to provide this algorithm with extra information about uncertainty such as detection accuracy and occlusions, to generate safe driving policies.&lt;/p>
&lt;p>We use two environments for training and evaluation, a 2D gym environment and a realistic 3D simulation environment (CARLA).&lt;/p>
&lt;p>&lt;strong>Task description:&lt;/strong>
During this project, you will carry out the following tasks:&lt;/p>
&lt;ul>
&lt;li>Implement and evaluate a baseline Distributional Reinforcement Learning (DRL) agent in the CARLA environment. For this task, you will need to integrate a scene understanding method such as semantic segmentation with a DRL agent.&lt;/li>
&lt;li>Train and evaluate a baseline agent in presence of observation uncertainties.&lt;/li>
&lt;li>Train and evlauate an agent provided with the observation uncertainty information.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Prerequisites:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Good programming and neural networks implementation skills.&lt;/li>
&lt;li>Theoretical knowledge about neural networks.&lt;/li>
&lt;li>General knowledge about reinforcement learning and/or high motivation to learn.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Interested?&lt;/strong> Send an email to &lt;a href="mailto:a.roomi.zadeh@tue.nl">a.roomi.zadeh@tue.nl&lt;/a>, containing:&lt;/p>
&lt;ul>
&lt;li>Brief motivation letter&lt;/li>
&lt;li>List of relevant courses and grades&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>References and further reading&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>[1] &lt;a href="https://www.youtube.com/watch?v=-L9VuPpzVdQ" target="_blank" rel="noopener">The CARLA Autonomous Driving Leaderboard&lt;/a>&lt;/li>
&lt;li>[2] M.G. Bellemare, W. Dabney, R. Munos, &amp;ldquo;A Distributional Perspective on Reinforcement Learning&amp;rdquo;, International Conference on Machine Learning 2017.&lt;/li>
&lt;/ul></description></item><item><title>Towards Modeling Vehicle Interactions for the purpose of Path Prediction</title><link>https://tue-mps.github.io/post/2021-09-internship-vehicle-interaction/</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://tue-mps.github.io/post/2021-09-internship-vehicle-interaction/</guid><description>&lt;p>&lt;strong>Important note: this project is provisional, and the details may be subject to change.&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Summary:&lt;/strong> During this project, you will be able to model the interactions between vehicles to improve trajectory prediction of those vehicles.&lt;/p>
&lt;p>&lt;strong>Type:&lt;/strong> Internship&lt;/p>
&lt;p>&lt;strong>Starting date:&lt;/strong> September 2021&lt;/p>
&lt;p>&lt;strong>Supervisor:&lt;/strong> Ariyan Bighashdel&lt;/p>
&lt;p>&lt;strong>General description:&lt;/strong>&lt;/p>
&lt;p>Developing Advanced Driving Assistance Systems (ADAS) can play a crucial role in reducing the number of injuries and deaths, related to motor vehicle accidents. Although this is encouraging, ADAS still have significant challenges when it comes to driving in more crowded
areas. In these situations, forecasting the future movements of vehicles is a vital task in producing collision-free paths. However, trajectory prediction of the vehicles has remained a complex problem due to numerous factors, one of which is the interactions between the vehicles.&lt;/p>
&lt;p>In the last decades, the subject of interaction modeling has been extensively addressed in the literature for pedestrian path prediction purposes. For many years, researchers heavily relied on traditional approaches with hand-crafted models to interpret the interactions between humans in crowded areas. After the dawn of deep learning, researchers had the opportunity to rely on more learning-based methods to infer the interactions. The developed learning-based methods have shown promising results for pedestrian path prediction in various scenarios with dominating human-human interactions.&lt;/p>
&lt;p>To date, few studies have examined the association between human-human and vehicle-vehicle interaction modeling methods. Therefore, the goal of this project is to develop a vehicle trajectory prediction framework with various interaction modeling methods which are inspired from human-human interaction modeling, and compare them in a well-controlled study.&lt;/p>
&lt;p>&lt;strong>Task description:&lt;/strong>
During this project, you will accomplish the following goals:&lt;/p>
&lt;ul>
&lt;li>Implementing a vehicle path prediction model with an encoder-decoder LSTM network.&lt;/li>
&lt;li>Implementing three human-human interaction modeling methods: 1) discrete social pooling [1] 2) continuous social pooling [2] and 3) graph attention [3], in the vehicle path prediction model.&lt;/li>
&lt;li>Evaluation and comparison of the three interaction modeling methods on the rounD vehicle trajectory dataset [4].&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Prerequisites:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Experience with the implementation of recurrent neural networks.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Interested?&lt;/strong> Send an email to &lt;a href="mailto:a.bighashdel@tue.nl">a.bighashdel@tue.nl&lt;/a>, containing:&lt;/p>
&lt;ul>
&lt;li>Brief motivation letter&lt;/li>
&lt;li>List of relevant courses and grades&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>References&lt;/strong>&lt;/p>
&lt;p>[1] A. Alahi, K. Goel, V. Ramanathan, A. Robicquet, L. Fei-Fei, and S. Savarese, &amp;ldquo;Social lstm: Human trajectory prediction in crowded spaces,&amp;rdquo; in Computer Pision and Pattern Recognition (CVPR), 2016, pp. 961-971.&lt;/p>
&lt;p>[2] A. Gupta, J. Johnson, L. Fei-Fei, S. Savarese, and A. Alahi, &amp;ldquo;Social gan: Socially acceptable trajectories with generative adversarial networks,&amp;rdquo; in Computer Pision and Pattern Recognition (CVPR), 2018, pp. 2255-2264.&lt;/p>
&lt;p>[3] Y. Huang, H. Bi, Z. Li, T. Mao, and Z. Wang, &amp;ldquo;Stgat: Modeling spatial-temporal interactions for human trajectory prediction,&amp;rdquo; in International Conference on Computer Vision (ICCV), 2019, pp. 6272-6281.&lt;/p>
&lt;p>[4] R. Krajewski, T. Moers, J. Bock, L. Vater, and L. Eckstein, &amp;ldquo;The rounD Dataset: A Drone Dataset of Road User Trajectories at Roundabouts in Germany,&amp;rdquo; in International Conference on Intelligent Transportation Systems (ITSC), 2020, pp. 1-6.&lt;/p></description></item><item><title>Pedestrian Path Prediction via Imitation Learning with Inverse Reinforcement Learning</title><link>https://tue-mps.github.io/post/2021-09-internship-trajectory-prediction/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://tue-mps.github.io/post/2021-09-internship-trajectory-prediction/</guid><description>&lt;p>&lt;strong>Important note: this project is provisional, and the details may be subject to change.&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Summary:&lt;/strong> During this project, you will apply a state-of-the-art imitation learning algorithm to the task of pedestrian path prediction.&lt;/p>
&lt;p>&lt;strong>Type:&lt;/strong> Internship&lt;/p>
&lt;p>&lt;strong>Starting date:&lt;/strong> September 2021&lt;/p>
&lt;p>&lt;strong>Supervisor:&lt;/strong> Ariyan Bighashdel&lt;/p>
&lt;p>&lt;strong>General description:&lt;/strong>&lt;/p>
&lt;p>Pedestrian path prediction is an important problem in computer vision, with various applications such as autonomous vehicles and video surveillance. Despite significant progress over the past years, pedestrian path prediction has remained a complex problem, far from being solved.&lt;/p>
&lt;p>The problem of pedestrian path prediction in the literature has been formulated in various ways, but conceptually all try to answer the following simple question:&lt;/p>
&lt;p>&amp;ldquo;&lt;em>Observing available information for a specific period of time, what will be the future positions of a target pedestrian in the next period of time?&lt;/em>&amp;rdquo;&lt;/p>
&lt;p>The proposed formulations may fundamentally differ in providing an answer to the above question. Path prediction approaches can be generally formulated as either a supervised learning (SL) problem or an imitation learning (IL) one. In the SL formulation, the idea is to train a (deep) model with a set of training input-output pairs. On the other hand, in the IL formulation, the goal is to train an agent to mimic the behavior of pedestrian from a set of (expert) pedestrian demonstrations.&lt;/p>
&lt;p>To date, the SL formulation of the pedestrian path prediction task has received scant attention in the research literature and there is a surprisingly little amount of studies comparing two proposed formulations. Therefore, the goal is to compare both formulations in a well-controlled study.&lt;/p>
&lt;p>&lt;strong>Task description:&lt;/strong>
During this project, you will accomplish the following goals:&lt;/p>
&lt;ul>
&lt;li>Running an available pedestrian path prediction model with the encoder-decoder LSTM network.&lt;/li>
&lt;li>Running an available deep reinforcement learning algorithm for a general robotics task.&lt;/li>
&lt;li>Implementation of a pedestrian path prediction model using Generative Adversarial Imitation Learning [1].&lt;/li>
&lt;li>Evaluation and comparison of the two prediction models in a well-controlled study.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Prerequisites:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Experience with implementation of recurrent neural networks.&lt;/li>
&lt;li>General knowledge about reinforcement learning and/or high motivation to learn.&lt;/li>
&lt;li>Familiarity with simulation environments, (e.g. gym) and/or high motivation to learn&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Interested?&lt;/strong> Send an email to &lt;a href="mailto:a.bighashdel@tue.nl">a.bighashdel@tue.nl&lt;/a>, containing:&lt;/p>
&lt;ul>
&lt;li>Brief motivation letter&lt;/li>
&lt;li>List of relevant courses and grades&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>References&lt;/strong>&lt;/p>
&lt;p>[1] J. Ho and S. Ermon, “Generative adversarial imitation learning,” in Advances in Neural Information Processing Systems, 2016, pp. 4565–4573.&lt;/p></description></item><item><title>Accuracy and Efficiency Improvements for Fast Panoptic Segmentation</title><link>https://tue-mps.github.io/post/2021-09-internship-panoptic-segmentation/</link><pubDate>Fri, 09 Jul 2021 00:00:00 +0000</pubDate><guid>https://tue-mps.github.io/post/2021-09-internship-panoptic-segmentation/</guid><description>&lt;p>&lt;strong>Update: this project is not available anymore. It remains visible to show the type of student projects available at our group.&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Summary:&lt;/strong> During this project, you will improve an existing network for panoptic segmentation, to make it more accurate and efficient.&lt;/p>
&lt;p>&lt;strong>Type:&lt;/strong> Internship&lt;/p>
&lt;p>&lt;strong>Starting date:&lt;/strong> September 2021&lt;/p>
&lt;p>&lt;strong>Supervisor:&lt;/strong> Daan de Geus&lt;/p>
&lt;p>&lt;strong>General description:&lt;/strong>&lt;/p>
&lt;p>Self-driving vehicles need situational awareness to be able to take appropriate actions. One way of gaining situational awareness is by applying scene understanding algorithms to cameras mounted on the vehicle. Panoptic segmentation is such a scene understanding task, which aims at recognizing all entities in a scene. Specifically, the goal is to predict, for each pixel, 1) a scene-class label (&lt;em>e.g.&lt;/em>, car, person, sky), 2) an instance &lt;em>id&lt;/em> to distinguish between individual objects of countable classes (&lt;em>e.g.&lt;/em>, individual cars or persons).&lt;/p>
&lt;p>Over the past years, several deep neural networks have been developed for panoptic segmentation, with varying accuracies and efficiencies. Especially for self-driving vehicles, where real-time applicability is key, efficiency of neural networks is very important. For this purpose, the Fast Panoptic Segmentation Network (FPSNet) was introduced [1].
FPSNet achieves efficient panoptic segmentation by making use of a fast object detection backbone, and an attention mechanism that indicates the location of individual objects.&lt;/p>
&lt;p>However, recent work has outperformed FPSNet in terms of accuracy and efficiency, making use of various novel techniques. The goal of this internship is to leverage some of these techniques to improve the efficiency and accuracy of FPSNet.&lt;/p>
&lt;p>&lt;strong>Task description:&lt;/strong>
During this project, you will implement and evaluate several improvements to FPSNet. These improvements include:&lt;/p>
&lt;ul>
&lt;li>More supervision of objects attended by attention masks.&lt;/li>
&lt;li>More accurate attention masks, instead of using bell-shaped blobs. These could be generated by letting the network learn &lt;em>borderness&lt;/em> of objects.&lt;/li>
&lt;li>More advanced panoptic head architecture.&lt;/li>
&lt;li>More efficient and accurate detection backbone.&lt;/li>
&lt;li>Further optimization of hyperparameters and learning strategy.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Prerequisites:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Theoretical knowledge about deep neural networks for computer vision.&lt;/li>
&lt;li>Experience with implementing a deep neural network for computer vision.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Interested?&lt;/strong> Send an email to &lt;a href="mailto:d.c.d.geus@tue.nl">d.c.d.geus@tue.nl&lt;/a>, containing:&lt;/p>
&lt;ul>
&lt;li>Brief motivation letter&lt;/li>
&lt;li>List of relevant courses and grades&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>References&lt;/strong>&lt;/p>
&lt;p>[1] D. de Geus, P. Meletis and G. Dubbelman, &amp;ldquo;Fast Panoptic Segmentation Network&amp;rdquo;, IEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 1742-1749, 2020.&lt;/p></description></item></channel></rss>